{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다변수의 time series LSTM 예측-(1)Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from datetime import datetime\n",
    "from pandas import DataFrame\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodings import euc_kr\n",
    "m=pd.read_csv('2019빅데이터페스티벌데이터/market_noDram_final.csv',encoding='euc_kr')\n",
    "k=pd.read_csv('2019빅데이터페스티벌데이터/KOSPI_국면분석.csv', encoding='euc_kr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kospi_after_5 = k['지수종가'].shift(-5)\n",
    "k['kospi_after_5']=kospi_after_5\n",
    "k2=k[['날짜','kospi_after_5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>kospi_after_5</th>\n",
       "      <th>A등급 구리(현물)</th>\n",
       "      <th>니켈(현물)</th>\n",
       "      <th>알루미늄 99.7%(현물)</th>\n",
       "      <th>금괴(일간)</th>\n",
       "      <th>미국1년</th>\n",
       "      <th>미국3년</th>\n",
       "      <th>미국10년</th>\n",
       "      <th>국고채권(1년)</th>\n",
       "      <th>국고채권(3년)</th>\n",
       "      <th>국고채권(10년)</th>\n",
       "      <th>브랜트유 가격</th>\n",
       "      <th>원유 WTI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-02</td>\n",
       "      <td>589.92</td>\n",
       "      <td>1746.3</td>\n",
       "      <td>6935.0</td>\n",
       "      <td>1552.5</td>\n",
       "      <td>270.6</td>\n",
       "      <td>5.20</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.7</td>\n",
       "      <td>7.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>27.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-01-03</td>\n",
       "      <td>560.81</td>\n",
       "      <td>1716.8</td>\n",
       "      <td>6370.0</td>\n",
       "      <td>1515.5</td>\n",
       "      <td>267.5</td>\n",
       "      <td>5.10</td>\n",
       "      <td>4.90</td>\n",
       "      <td>5.2</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7.2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>28.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-01-04</td>\n",
       "      <td>561.79</td>\n",
       "      <td>1750.5</td>\n",
       "      <td>6569.0</td>\n",
       "      <td>1519.0</td>\n",
       "      <td>267.7</td>\n",
       "      <td>4.90</td>\n",
       "      <td>4.80</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>6.4</td>\n",
       "      <td>6.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>28.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-01-05</td>\n",
       "      <td>587.87</td>\n",
       "      <td>1740.3</td>\n",
       "      <td>6607.0</td>\n",
       "      <td>1527.5</td>\n",
       "      <td>268.2</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.9</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.7</td>\n",
       "      <td>25.2</td>\n",
       "      <td>28.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-01-08</td>\n",
       "      <td>599.00</td>\n",
       "      <td>1775.0</td>\n",
       "      <td>6734.0</td>\n",
       "      <td>1578.0</td>\n",
       "      <td>268.3</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.9</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.6</td>\n",
       "      <td>24.4</td>\n",
       "      <td>27.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2001-01-09</td>\n",
       "      <td>603.42</td>\n",
       "      <td>1783.3</td>\n",
       "      <td>6796.0</td>\n",
       "      <td>1572.3</td>\n",
       "      <td>267.4</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.70</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>24.6</td>\n",
       "      <td>27.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2001-01-10</td>\n",
       "      <td>595.83</td>\n",
       "      <td>1779.8</td>\n",
       "      <td>6947.0</td>\n",
       "      <td>1591.0</td>\n",
       "      <td>266.3</td>\n",
       "      <td>4.90</td>\n",
       "      <td>4.80</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>25.3</td>\n",
       "      <td>29.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2001-01-11</td>\n",
       "      <td>604.05</td>\n",
       "      <td>1795.0</td>\n",
       "      <td>7140.0</td>\n",
       "      <td>1597.5</td>\n",
       "      <td>263.9</td>\n",
       "      <td>4.90</td>\n",
       "      <td>4.80</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>25.6</td>\n",
       "      <td>29.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2001-01-12</td>\n",
       "      <td>619.78</td>\n",
       "      <td>1807.5</td>\n",
       "      <td>7325.0</td>\n",
       "      <td>1632.5</td>\n",
       "      <td>264.3</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>5.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>25.8</td>\n",
       "      <td>30.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2001-01-15</td>\n",
       "      <td>627.45</td>\n",
       "      <td>1800.8</td>\n",
       "      <td>7145.0</td>\n",
       "      <td>1626.0</td>\n",
       "      <td>263.6</td>\n",
       "      <td>5.18</td>\n",
       "      <td>5.05</td>\n",
       "      <td>5.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>26.2</td>\n",
       "      <td>30.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2001-01-16</td>\n",
       "      <td>591.73</td>\n",
       "      <td>1754.0</td>\n",
       "      <td>6995.0</td>\n",
       "      <td>1587.0</td>\n",
       "      <td>264.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.90</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.6</td>\n",
       "      <td>26.2</td>\n",
       "      <td>30.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2001-01-17</td>\n",
       "      <td>596.54</td>\n",
       "      <td>1776.0</td>\n",
       "      <td>7320.0</td>\n",
       "      <td>1609.0</td>\n",
       "      <td>263.6</td>\n",
       "      <td>4.90</td>\n",
       "      <td>4.80</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6.6</td>\n",
       "      <td>24.8</td>\n",
       "      <td>29.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2001-01-18</td>\n",
       "      <td>591.34</td>\n",
       "      <td>1835.0</td>\n",
       "      <td>7415.0</td>\n",
       "      <td>1661.0</td>\n",
       "      <td>264.7</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.70</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6.5</td>\n",
       "      <td>25.6</td>\n",
       "      <td>29.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2001-01-19</td>\n",
       "      <td>617.91</td>\n",
       "      <td>1823.0</td>\n",
       "      <td>7278.0</td>\n",
       "      <td>1648.5</td>\n",
       "      <td>264.6</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.80</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6.5</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2001-01-22</td>\n",
       "      <td>612.30</td>\n",
       "      <td>1834.8</td>\n",
       "      <td>7265.0</td>\n",
       "      <td>1642.5</td>\n",
       "      <td>265.9</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.80</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6.5</td>\n",
       "      <td>26.5</td>\n",
       "      <td>32.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2001-01-26</td>\n",
       "      <td>608.48</td>\n",
       "      <td>1796.0</td>\n",
       "      <td>6720.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>263.8</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.80</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6.6</td>\n",
       "      <td>27.0</td>\n",
       "      <td>29.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2001-01-29</td>\n",
       "      <td>579.16</td>\n",
       "      <td>1781.5</td>\n",
       "      <td>6780.0</td>\n",
       "      <td>1686.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.80</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6.6</td>\n",
       "      <td>26.6</td>\n",
       "      <td>29.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2001-01-30</td>\n",
       "      <td>586.58</td>\n",
       "      <td>1791.0</td>\n",
       "      <td>6935.0</td>\n",
       "      <td>1711.0</td>\n",
       "      <td>266.2</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.80</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6.6</td>\n",
       "      <td>26.9</td>\n",
       "      <td>29.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2001-01-31</td>\n",
       "      <td>576.19</td>\n",
       "      <td>1799.8</td>\n",
       "      <td>7020.0</td>\n",
       "      <td>1716.0</td>\n",
       "      <td>265.2</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.70</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>6.5</td>\n",
       "      <td>26.7</td>\n",
       "      <td>28.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2001-02-01</td>\n",
       "      <td>591.57</td>\n",
       "      <td>1804.8</td>\n",
       "      <td>6845.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>267.4</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.60</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>6.5</td>\n",
       "      <td>28.1</td>\n",
       "      <td>29.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2001-02-02</td>\n",
       "      <td>595.47</td>\n",
       "      <td>1795.0</td>\n",
       "      <td>6920.0</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>267.2</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.70</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>29.2</td>\n",
       "      <td>31.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2001-02-05</td>\n",
       "      <td>599.20</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>6850.0</td>\n",
       "      <td>1653.3</td>\n",
       "      <td>265.7</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.70</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.4</td>\n",
       "      <td>6.5</td>\n",
       "      <td>28.5</td>\n",
       "      <td>30.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2001-02-06</td>\n",
       "      <td>598.78</td>\n",
       "      <td>1774.0</td>\n",
       "      <td>6870.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>263.3</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.80</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.3</td>\n",
       "      <td>6.4</td>\n",
       "      <td>28.7</td>\n",
       "      <td>30.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2001-02-07</td>\n",
       "      <td>603.83</td>\n",
       "      <td>1778.0</td>\n",
       "      <td>6900.0</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>263.2</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.70</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>6.3</td>\n",
       "      <td>29.9</td>\n",
       "      <td>31.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2001-02-08</td>\n",
       "      <td>603.63</td>\n",
       "      <td>1779.5</td>\n",
       "      <td>6850.0</td>\n",
       "      <td>1642.5</td>\n",
       "      <td>263.1</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.80</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>29.8</td>\n",
       "      <td>31.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2001-02-09</td>\n",
       "      <td>604.92</td>\n",
       "      <td>1763.5</td>\n",
       "      <td>6490.0</td>\n",
       "      <td>1627.3</td>\n",
       "      <td>260.2</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.70</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.2</td>\n",
       "      <td>6.1</td>\n",
       "      <td>29.3</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2001-02-12</td>\n",
       "      <td>596.67</td>\n",
       "      <td>1761.0</td>\n",
       "      <td>6586.0</td>\n",
       "      <td>1618.0</td>\n",
       "      <td>261.5</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.70</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2001-02-13</td>\n",
       "      <td>608.74</td>\n",
       "      <td>1778.3</td>\n",
       "      <td>6667.0</td>\n",
       "      <td>1614.5</td>\n",
       "      <td>261.1</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.80</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>30.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2001-02-14</td>\n",
       "      <td>594.53</td>\n",
       "      <td>1765.3</td>\n",
       "      <td>6507.0</td>\n",
       "      <td>1601.0</td>\n",
       "      <td>260.7</td>\n",
       "      <td>4.80</td>\n",
       "      <td>4.90</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.3</td>\n",
       "      <td>29.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2001-02-15</td>\n",
       "      <td>583.41</td>\n",
       "      <td>1784.5</td>\n",
       "      <td>6424.0</td>\n",
       "      <td>1612.3</td>\n",
       "      <td>258.5</td>\n",
       "      <td>4.90</td>\n",
       "      <td>4.90</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>28.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>2018-11-16</td>\n",
       "      <td>2057.48</td>\n",
       "      <td>6223.5</td>\n",
       "      <td>11287.0</td>\n",
       "      <td>1921.3</td>\n",
       "      <td>1220.9</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>67.1</td>\n",
       "      <td>56.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4420</th>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>2083.02</td>\n",
       "      <td>6292.5</td>\n",
       "      <td>11108.0</td>\n",
       "      <td>1916.5</td>\n",
       "      <td>1222.8</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>67.0</td>\n",
       "      <td>56.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4421</th>\n",
       "      <td>2018-11-20</td>\n",
       "      <td>2099.42</td>\n",
       "      <td>6206.0</td>\n",
       "      <td>11027.0</td>\n",
       "      <td>1929.3</td>\n",
       "      <td>1224.2</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>53.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4422</th>\n",
       "      <td>2018-11-21</td>\n",
       "      <td>2108.22</td>\n",
       "      <td>6269.0</td>\n",
       "      <td>10951.5</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>1226.6</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>63.5</td>\n",
       "      <td>54.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4423</th>\n",
       "      <td>2018-11-22</td>\n",
       "      <td>2114.10</td>\n",
       "      <td>6286.3</td>\n",
       "      <td>10902.3</td>\n",
       "      <td>1936.8</td>\n",
       "      <td>1227.8</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>62.5</td>\n",
       "      <td>54.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4424</th>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>2096.86</td>\n",
       "      <td>6236.5</td>\n",
       "      <td>10849.0</td>\n",
       "      <td>1938.3</td>\n",
       "      <td>1223.2</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>59.2</td>\n",
       "      <td>50.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4425</th>\n",
       "      <td>2018-11-26</td>\n",
       "      <td>2131.93</td>\n",
       "      <td>6221.0</td>\n",
       "      <td>10803.5</td>\n",
       "      <td>1940.5</td>\n",
       "      <td>1223.3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>60.4</td>\n",
       "      <td>51.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4426</th>\n",
       "      <td>2018-11-27</td>\n",
       "      <td>2114.35</td>\n",
       "      <td>6155.3</td>\n",
       "      <td>10700.0</td>\n",
       "      <td>1920.5</td>\n",
       "      <td>1213.2</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>60.9</td>\n",
       "      <td>51.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4427</th>\n",
       "      <td>2018-11-28</td>\n",
       "      <td>2101.31</td>\n",
       "      <td>6244.0</td>\n",
       "      <td>10726.5</td>\n",
       "      <td>1924.8</td>\n",
       "      <td>1213.5</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>58.6</td>\n",
       "      <td>50.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4428</th>\n",
       "      <td>2018-11-29</td>\n",
       "      <td>2068.69</td>\n",
       "      <td>6247.5</td>\n",
       "      <td>10984.0</td>\n",
       "      <td>1936.8</td>\n",
       "      <td>1227.4</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>59.3</td>\n",
       "      <td>51.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4429</th>\n",
       "      <td>2018-11-30</td>\n",
       "      <td>2075.76</td>\n",
       "      <td>6227.0</td>\n",
       "      <td>11135.5</td>\n",
       "      <td>1957.3</td>\n",
       "      <td>1219.2</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>58.7</td>\n",
       "      <td>50.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430</th>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>2053.79</td>\n",
       "      <td>6312.5</td>\n",
       "      <td>11179.5</td>\n",
       "      <td>1972.3</td>\n",
       "      <td>1234.2</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>61.9</td>\n",
       "      <td>53.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4431</th>\n",
       "      <td>2018-12-04</td>\n",
       "      <td>2052.97</td>\n",
       "      <td>6212.5</td>\n",
       "      <td>11080.5</td>\n",
       "      <td>1972.5</td>\n",
       "      <td>1238.4</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>61.3</td>\n",
       "      <td>53.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432</th>\n",
       "      <td>2018-12-05</td>\n",
       "      <td>2082.57</td>\n",
       "      <td>6175.0</td>\n",
       "      <td>11156.0</td>\n",
       "      <td>1970.3</td>\n",
       "      <td>1237.8</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>61.6</td>\n",
       "      <td>52.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4433</th>\n",
       "      <td>2018-12-06</td>\n",
       "      <td>2095.55</td>\n",
       "      <td>6079.5</td>\n",
       "      <td>10780.5</td>\n",
       "      <td>1937.0</td>\n",
       "      <td>1242.9</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.3</td>\n",
       "      <td>51.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4434</th>\n",
       "      <td>2018-12-07</td>\n",
       "      <td>2069.38</td>\n",
       "      <td>6149.0</td>\n",
       "      <td>10845.5</td>\n",
       "      <td>1950.8</td>\n",
       "      <td>1245.8</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>61.4</td>\n",
       "      <td>52.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4435</th>\n",
       "      <td>2018-12-10</td>\n",
       "      <td>2071.09</td>\n",
       "      <td>6088.5</td>\n",
       "      <td>10729.5</td>\n",
       "      <td>1928.8</td>\n",
       "      <td>1246.7</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>59.8</td>\n",
       "      <td>51.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4436</th>\n",
       "      <td>2018-12-11</td>\n",
       "      <td>2062.11</td>\n",
       "      <td>6170.5</td>\n",
       "      <td>10696.5</td>\n",
       "      <td>1927.5</td>\n",
       "      <td>1244.2</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.5</td>\n",
       "      <td>51.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4437</th>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>2078.84</td>\n",
       "      <td>6144.8</td>\n",
       "      <td>10727.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>1246.2</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.2</td>\n",
       "      <td>51.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4438</th>\n",
       "      <td>2018-12-13</td>\n",
       "      <td>2060.12</td>\n",
       "      <td>6158.5</td>\n",
       "      <td>10780.5</td>\n",
       "      <td>1913.8</td>\n",
       "      <td>1242.7</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>61.7</td>\n",
       "      <td>52.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4439</th>\n",
       "      <td>2018-12-14</td>\n",
       "      <td>2061.49</td>\n",
       "      <td>6127.8</td>\n",
       "      <td>11009.5</td>\n",
       "      <td>1908.3</td>\n",
       "      <td>1238.7</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60.3</td>\n",
       "      <td>51.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4440</th>\n",
       "      <td>2018-12-17</td>\n",
       "      <td>2055.01</td>\n",
       "      <td>6095.5</td>\n",
       "      <td>10907.5</td>\n",
       "      <td>1943.3</td>\n",
       "      <td>1242.6</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>58.7</td>\n",
       "      <td>49.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4441</th>\n",
       "      <td>2018-12-18</td>\n",
       "      <td>2028.01</td>\n",
       "      <td>5944.8</td>\n",
       "      <td>10752.5</td>\n",
       "      <td>1920.8</td>\n",
       "      <td>1246.4</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.1</td>\n",
       "      <td>46.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4442</th>\n",
       "      <td>2018-12-19</td>\n",
       "      <td>2028.44</td>\n",
       "      <td>5990.5</td>\n",
       "      <td>10868.0</td>\n",
       "      <td>1927.5</td>\n",
       "      <td>1255.9</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>56.6</td>\n",
       "      <td>47.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4443</th>\n",
       "      <td>2018-12-20</td>\n",
       "      <td>2041.04</td>\n",
       "      <td>5977.5</td>\n",
       "      <td>10832.0</td>\n",
       "      <td>1911.3</td>\n",
       "      <td>1258.1</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>54.7</td>\n",
       "      <td>45.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4444</th>\n",
       "      <td>2018-12-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5967.5</td>\n",
       "      <td>10796.0</td>\n",
       "      <td>1907.8</td>\n",
       "      <td>1259.9</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.5</td>\n",
       "      <td>45.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4445</th>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5931.8</td>\n",
       "      <td>10801.0</td>\n",
       "      <td>1897.8</td>\n",
       "      <td>1265.5</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>50.5</td>\n",
       "      <td>42.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4446</th>\n",
       "      <td>2018-12-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5931.8</td>\n",
       "      <td>10801.0</td>\n",
       "      <td>1897.8</td>\n",
       "      <td>1278.6</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>54.9</td>\n",
       "      <td>46.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4447</th>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5969.0</td>\n",
       "      <td>10673.0</td>\n",
       "      <td>1856.5</td>\n",
       "      <td>1275.3</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>53.0</td>\n",
       "      <td>44.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4448</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5987.0</td>\n",
       "      <td>10647.0</td>\n",
       "      <td>1856.8</td>\n",
       "      <td>1277.5</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>52.2</td>\n",
       "      <td>45.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4449 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              날짜  kospi_after_5  A등급 구리(현물)   니켈(현물)  알루미늄 99.7%(현물)  금괴(일간)  \\\n",
       "0     2001-01-02         589.92      1746.3   6935.0          1552.5   270.6   \n",
       "1     2001-01-03         560.81      1716.8   6370.0          1515.5   267.5   \n",
       "2     2001-01-04         561.79      1750.5   6569.0          1519.0   267.7   \n",
       "3     2001-01-05         587.87      1740.3   6607.0          1527.5   268.2   \n",
       "4     2001-01-08         599.00      1775.0   6734.0          1578.0   268.3   \n",
       "5     2001-01-09         603.42      1783.3   6796.0          1572.3   267.4   \n",
       "6     2001-01-10         595.83      1779.8   6947.0          1591.0   266.3   \n",
       "7     2001-01-11         604.05      1795.0   7140.0          1597.5   263.9   \n",
       "8     2001-01-12         619.78      1807.5   7325.0          1632.5   264.3   \n",
       "9     2001-01-15         627.45      1800.8   7145.0          1626.0   263.6   \n",
       "10    2001-01-16         591.73      1754.0   6995.0          1587.0   264.5   \n",
       "11    2001-01-17         596.54      1776.0   7320.0          1609.0   263.6   \n",
       "12    2001-01-18         591.34      1835.0   7415.0          1661.0   264.7   \n",
       "13    2001-01-19         617.91      1823.0   7278.0          1648.5   264.6   \n",
       "14    2001-01-22         612.30      1834.8   7265.0          1642.5   265.9   \n",
       "15    2001-01-26         608.48      1796.0   6720.0          1700.0   263.8   \n",
       "16    2001-01-29         579.16      1781.5   6780.0          1686.0   263.0   \n",
       "17    2001-01-30         586.58      1791.0   6935.0          1711.0   266.2   \n",
       "18    2001-01-31         576.19      1799.8   7020.0          1716.0   265.2   \n",
       "19    2001-02-01         591.57      1804.8   6845.0          1700.0   267.4   \n",
       "20    2001-02-02         595.47      1795.0   6920.0          1687.0   267.2   \n",
       "21    2001-02-05         599.20      1777.0   6850.0          1653.3   265.7   \n",
       "22    2001-02-06         598.78      1774.0   6870.0          1620.0   263.3   \n",
       "23    2001-02-07         603.83      1778.0   6900.0          1619.0   263.2   \n",
       "24    2001-02-08         603.63      1779.5   6850.0          1642.5   263.1   \n",
       "25    2001-02-09         604.92      1763.5   6490.0          1627.3   260.2   \n",
       "26    2001-02-12         596.67      1761.0   6586.0          1618.0   261.5   \n",
       "27    2001-02-13         608.74      1778.3   6667.0          1614.5   261.1   \n",
       "28    2001-02-14         594.53      1765.3   6507.0          1601.0   260.7   \n",
       "29    2001-02-15         583.41      1784.5   6424.0          1612.3   258.5   \n",
       "...          ...            ...         ...      ...             ...     ...   \n",
       "4419  2018-11-16        2057.48      6223.5  11287.0          1921.3  1220.9   \n",
       "4420  2018-11-19        2083.02      6292.5  11108.0          1916.5  1222.8   \n",
       "4421  2018-11-20        2099.42      6206.0  11027.0          1929.3  1224.2   \n",
       "4422  2018-11-21        2108.22      6269.0  10951.5          1943.0  1226.6   \n",
       "4423  2018-11-22        2114.10      6286.3  10902.3          1936.8  1227.8   \n",
       "4424  2018-11-23        2096.86      6236.5  10849.0          1938.3  1223.2   \n",
       "4425  2018-11-26        2131.93      6221.0  10803.5          1940.5  1223.3   \n",
       "4426  2018-11-27        2114.35      6155.3  10700.0          1920.5  1213.2   \n",
       "4427  2018-11-28        2101.31      6244.0  10726.5          1924.8  1213.5   \n",
       "4428  2018-11-29        2068.69      6247.5  10984.0          1936.8  1227.4   \n",
       "4429  2018-11-30        2075.76      6227.0  11135.5          1957.3  1219.2   \n",
       "4430  2018-12-03        2053.79      6312.5  11179.5          1972.3  1234.2   \n",
       "4431  2018-12-04        2052.97      6212.5  11080.5          1972.5  1238.4   \n",
       "4432  2018-12-05        2082.57      6175.0  11156.0          1970.3  1237.8   \n",
       "4433  2018-12-06        2095.55      6079.5  10780.5          1937.0  1242.9   \n",
       "4434  2018-12-07        2069.38      6149.0  10845.5          1950.8  1245.8   \n",
       "4435  2018-12-10        2071.09      6088.5  10729.5          1928.8  1246.7   \n",
       "4436  2018-12-11        2062.11      6170.5  10696.5          1927.5  1244.2   \n",
       "4437  2018-12-12        2078.84      6144.8  10727.0          1922.0  1246.2   \n",
       "4438  2018-12-13        2060.12      6158.5  10780.5          1913.8  1242.7   \n",
       "4439  2018-12-14        2061.49      6127.8  11009.5          1908.3  1238.7   \n",
       "4440  2018-12-17        2055.01      6095.5  10907.5          1943.3  1242.6   \n",
       "4441  2018-12-18        2028.01      5944.8  10752.5          1920.8  1246.4   \n",
       "4442  2018-12-19        2028.44      5990.5  10868.0          1927.5  1255.9   \n",
       "4443  2018-12-20        2041.04      5977.5  10832.0          1911.3  1258.1   \n",
       "4444  2018-12-21            NaN      5967.5  10796.0          1907.8  1259.9   \n",
       "4445  2018-12-24            NaN      5931.8  10801.0          1897.8  1265.5   \n",
       "4446  2018-12-26            NaN      5931.8  10801.0          1897.8  1278.6   \n",
       "4447  2018-12-27            NaN      5969.0  10673.0          1856.5  1275.3   \n",
       "4448  2018-12-28            NaN      5987.0  10647.0          1856.8  1277.5   \n",
       "\n",
       "      미국1년  미국3년  미국10년  국고채권(1년)  국고채권(3년)  국고채권(10년)  브랜트유 가격  원유 WTI  \n",
       "0     5.20  4.80    4.9       6.7       6.7        7.2     24.3   27.20  \n",
       "1     5.10  4.90    5.2       6.6       6.6        7.2     25.0   28.00  \n",
       "2     4.90  4.80    5.0       6.4       6.4        6.9     25.4   28.10  \n",
       "3     4.60  4.60    4.9       6.2       6.2        6.7     25.2   28.00  \n",
       "4     4.60  4.60    4.9       6.1       6.1        6.6     24.4   27.30  \n",
       "5     4.70  4.70    5.0       6.1       6.1        6.7     24.6   27.60  \n",
       "6     4.90  4.80    5.1       6.1       6.1        6.7     25.3   29.50  \n",
       "7     4.90  4.80    5.1       6.0       6.0        6.6     25.6   29.40  \n",
       "8     5.00  4.90    5.2       6.0       6.0        6.6     25.8   30.10  \n",
       "9     5.18  5.05    5.2       6.0       6.0        6.6     26.2   30.36  \n",
       "10    5.00  4.90    5.2       5.9       5.9        6.6     26.2   30.30  \n",
       "11    4.90  4.80    5.2       5.9       5.7        6.6     24.8   29.60  \n",
       "12    4.80  4.70    5.1       5.9       5.8        6.5     25.6   29.80  \n",
       "13    4.80  4.80    5.2       5.9       5.8        6.5     27.0   32.20  \n",
       "14    4.80  4.80    5.2       5.9       5.8        6.5     26.5   32.20  \n",
       "15    4.80  4.80    5.3       5.9       5.8        6.6     27.0   29.80  \n",
       "16    4.80  4.80    5.3       5.9       5.8        6.6     26.6   29.10  \n",
       "17    4.70  4.80    5.2       5.8       5.7        6.6     26.9   29.10  \n",
       "18    4.60  4.70    5.1       5.7       5.7        6.5     26.7   28.70  \n",
       "19    4.60  4.60    5.1       5.7       5.6        6.5     28.1   29.80  \n",
       "20    4.70  4.70    5.1       5.6       5.5        6.5     29.2   31.20  \n",
       "21    4.70  4.70    5.2       5.6       5.4        6.5     28.5   30.60  \n",
       "22    4.70  4.80    5.2       5.5       5.3        6.4     28.7   30.40  \n",
       "23    4.80  4.70    5.2       5.5       5.2        6.3     29.9   31.30  \n",
       "24    4.70  4.80    5.1       5.5       5.3        6.3     29.8   31.60  \n",
       "25    4.70  4.70    5.0       5.4       5.2        6.1     29.3   31.00  \n",
       "26    4.70  4.70    5.0       5.2       5.0        5.9     29.0   30.50  \n",
       "27    4.80  4.80    5.1       5.2       5.1        6.0     28.5   30.40  \n",
       "28    4.80  4.90    5.1       5.2       5.2        6.0     27.3   29.70  \n",
       "29    4.90  4.90    5.2       5.2       5.1        6.0     26.6   28.80  \n",
       "...    ...   ...    ...       ...       ...        ...      ...     ...  \n",
       "4419  2.70  2.90    3.1       1.8       1.9        2.2     67.1   56.50  \n",
       "4420  2.70  2.80    3.1       1.8       1.9        2.2     67.0   56.80  \n",
       "4421  2.70  2.80    3.0       1.9       1.9        2.2     62.4   53.40  \n",
       "4422  2.70  2.90    3.1       1.9       1.9        2.2     63.5   54.60  \n",
       "4423  2.70  2.90    3.1       1.9       1.9        2.2     62.5   54.61  \n",
       "4424  2.70  2.80    3.1       1.8       1.9        2.2     59.2   50.40  \n",
       "4425  2.70  2.90    3.1       1.8       1.9        2.2     60.4   51.60  \n",
       "4426  2.70  2.90    3.1       1.8       1.9        2.2     60.9   51.60  \n",
       "4427  2.70  2.80    3.0       1.8       1.9        2.2     58.6   50.30  \n",
       "4428  2.70  2.80    3.0       1.8       1.9        2.1     59.3   51.50  \n",
       "4429  2.70  2.80    3.0       1.8       1.9        2.1     58.7   50.90  \n",
       "4430  2.70  2.80    3.0       1.8       1.9        2.1     61.9   53.00  \n",
       "4431  2.70  2.80    2.9       1.8       1.9        2.1     61.3   53.30  \n",
       "4432  2.70  2.80    2.9       1.8       1.9        2.1     61.6   52.90  \n",
       "4433  2.70  2.80    2.9       1.8       1.8        2.0     60.3   51.50  \n",
       "4434  2.70  2.70    2.9       1.8       1.8        2.0     61.4   52.60  \n",
       "4435  2.70  2.70    2.9       1.8       1.8        2.0     59.8   51.00  \n",
       "4436  2.70  2.80    2.9       1.8       1.8        2.0     60.5   51.70  \n",
       "4437  2.70  2.80    2.9       1.8       1.8        2.0     60.2   51.20  \n",
       "4438  2.70  2.80    2.9       1.8       1.8        2.0     61.7   52.60  \n",
       "4439  2.70  2.70    2.9       1.8       1.8        2.0     60.3   51.20  \n",
       "4440  2.70  2.70    2.9       1.8       1.8        2.0     58.7   49.90  \n",
       "4441  2.60  2.60    2.8       1.8       1.8        2.0     56.1   46.20  \n",
       "4442  2.60  2.60    2.8       1.7       1.8        1.9     56.6   47.20  \n",
       "4443  2.60  2.60    2.8       1.7       1.8        1.9     54.7   45.90  \n",
       "4444  2.60  2.60    2.8       1.7       1.8        2.0     53.5   45.60  \n",
       "4445  2.60  2.60    2.8       1.7       1.8        2.0     50.5   42.50  \n",
       "4446  2.60  2.60    2.8       1.7       1.8        1.9     54.9   46.20  \n",
       "4447  2.60  2.50    2.7       1.7       1.8        1.9     53.0   44.60  \n",
       "4448  2.60  2.50    2.7       1.7       1.8        1.9     52.2   45.30  \n",
       "\n",
       "[4449 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_m=pd.merge(left=k2, right=m,how='left', on=['날짜'])\n",
    "m_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You must specify a freq or x must be a pandas object with a timeseries index with a freq not set to None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-123-c715dc26ed76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mm3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'level_0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mm4\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mm3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3962\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseasonal_decompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkospi_after_5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#<statsmodels.tsa.seasonal.DecomposeResult object at 0x110ec3710>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\seasonal.py\u001b[0m in \u001b[0;36mseasonal_decompose\u001b[1;34m(x, model, filt, freq, two_sided, extrapolate_trend)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mfreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpfreq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             raise ValueError(\"You must specify a freq or x must be a \"\n\u001b[0m\u001b[0;32m    128\u001b[0m                              \u001b[1;34m\"pandas object with a timeseries index with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                              \"a freq not set to None\")\n",
      "\u001b[1;31mValueError\u001b[0m: You must specify a freq or x must be a pandas object with a timeseries index with a freq not set to None"
     ]
    }
   ],
   "source": [
    "#STL 분해 해보려고 함\n",
    "import statsmodels.api as sm\n",
    "\n",
    "m_m['날짜'] = pd.to_datetime(m_m['날짜'])\n",
    "m3 = m_m.set_index('날짜')\n",
    "m3.drop(['level_0', 'index'], axis=1)\n",
    "m4=m3.iloc[:3962,:]\n",
    "s=sm.tsa.seasonal_decompose(m4.kospi_after_5)\n",
    "#<statsmodels.tsa.seasonal.DecomposeResult object at 0x110ec3710>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2=m_m.drop('날짜', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(m2) #값을 0~1로 떨어뜨린다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "  n_vars = 1 if type(data) is list else data.shape[1]\n",
    "  df = DataFrame(data)\n",
    "  cols, names = list(), list()\n",
    "  # input sequence (t-n, ... t-1)\n",
    "  for i in range(n_in, 0, -1):\n",
    "      cols.append(df.shift(i))\n",
    "      names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "  # forecast sequence (t, t+1, ... t+n)\n",
    "  for i in range(0, n_out):\n",
    "      cols.append(df.shift(-i))\n",
    "      if i == 0:\n",
    "          names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "      else:\n",
    "          names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "  # put it all together\n",
    "  agg = pd.concat(cols, axis=1)\n",
    "  agg.columns = names\n",
    "  # drop rows with NaN values\n",
    "  if dropnan:\n",
    "      agg.dropna(inplace=True)\n",
    "  return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reframed = series_to_supervised(scaled, 1, 1) #t-1시점,t시점 데이터를 한 행으로 둔다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = reframed.values\n",
    "n_train = 3962 \n",
    "train = values[:n_train, :]\n",
    "test = values[n_train:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, 1:], train[:, 0]\n",
    "test_X, test_y = test[:, 1:], test[:, 0]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05689786, 0.04322753, 0.04368775, ...,        nan,        nan,\n",
       "              nan])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train[:,1:]\n",
    "scaled[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\chocolet\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(30, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Collecting keras\n",
      "  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "Requirement already satisfied: astor>=0.6.0 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.4)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorflow) (1.13.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorflow) (3.9.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorflow) (1.22.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorflow) (0.33.4)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorflow) (1.13.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from keras) (1.2.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from keras) (5.1.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.15.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n",
      "Requirement already satisfied: mock>=2.0.0 in c:\\users\\chocolet\\anaconda3\\lib\\site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow) (3.0.5)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3962 samples, validate on 481 samples\n",
      "Epoch 1/50\n",
      " - 4s - loss: 0.0955 - val_loss: 0.1404\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.2378 - val_loss: 0.1536\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.1328 - val_loss: 0.4038\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.1460 - val_loss: 0.2863\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.0766 - val_loss: 0.2336\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.0692 - val_loss: 0.2256\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.0672 - val_loss: 0.1954\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.0667 - val_loss: 0.1950\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.0645 - val_loss: 0.1754\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.0585 - val_loss: 0.1451\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.0575 - val_loss: 0.1286\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.0552 - val_loss: 0.1049\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.0524 - val_loss: 0.0898\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.0485 - val_loss: 0.0748\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.0463 - val_loss: 0.0691\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.0447 - val_loss: 0.0664\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.0423 - val_loss: 0.0660\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.0395 - val_loss: 0.0644\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.0421 - val_loss: 0.0661\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.0361 - val_loss: 0.0609\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.0407 - val_loss: 0.0627\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.0327 - val_loss: 0.0578\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.0379 - val_loss: 0.0592\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.0324 - val_loss: 0.0577\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.0333 - val_loss: 0.0554\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.0293 - val_loss: 0.0555\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.0297 - val_loss: 0.0539\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.0282 - val_loss: 0.0558\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.0284 - val_loss: 0.0549\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.0291 - val_loss: 0.0533\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.0322 - val_loss: 0.0517\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.0390 - val_loss: 0.0580\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.0505 - val_loss: 0.0795\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.0416 - val_loss: 0.0648\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.0336 - val_loss: 0.0555\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.0374 - val_loss: 0.0512\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.0350 - val_loss: 0.0500\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.0373 - val_loss: 0.0606\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.0285 - val_loss: 0.0657\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.0335 - val_loss: 0.0791\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.0267 - val_loss: 0.0879\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.0276 - val_loss: 0.0812\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.0336 - val_loss: 0.0780\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.0317 - val_loss: 0.0667\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.0321 - val_loss: 0.0594\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.0258 - val_loss: 0.0554\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.0254 - val_loss: 0.0434\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.0256 - val_loss: 0.0397\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.0281 - val_loss: 0.0348\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.0316 - val_loss: 0.0412\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, \n",
    "        validation_data=(test_X, test_y), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate\n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer concatenate_10 was called with an input that isn't a symbolic tensor. Received type: <class 'tuple'>. Full input: [(array([[0.7570466 ],\n       [0.7682475 ],\n       [0.7762338 ],\n       [0.77217567],\n       [0.7674738 ],\n       [0.76899123],\n       [0.7722814 ],\n       [0.77488005],\n       [0.7766211 ],\n       [0.7775085 ],\n       [0.77784705],\n       [0.7725867 ],\n       [0.7738311 ],\n       [0.7684125 ],\n       [0.77673686],\n       [0.7752683 ],\n       [0.77895474],\n       [0.7782291 ],\n       [0.7765517 ],\n       [0.776726  ],\n       [0.7762456 ],\n       [0.7770226 ],\n       [0.7781091 ],\n       [0.7771394 ],\n       [0.7824124 ],\n       [0.78234065],\n       [0.78467107],\n       [0.7862673 ],\n       [0.789443  ],\n       [0.79018676],\n       [0.7917814 ],\n       [0.78898954],\n       [0.7860291 ],\n       [0.7881701 ],\n       [0.79067934],\n       [0.7832844 ],\n       [0.78240407],\n       [0.78532517],\n       [0.78723836],\n       [0.78132427],\n       [0.7797041 ],\n       [0.7837982 ],\n       [0.78420436],\n       [0.77672017],\n       [0.7793063 ],\n       [0.7810577 ],\n       [0.77905595],\n       [0.78529227],\n       [0.78571105],\n       [0.79057133],\n       [0.793965  ],\n       [0.7901119 ],\n       [0.7909374 ],\n       [0.7891859 ],\n       [0.78872967],\n       [0.78789616],\n       [0.7899612 ],\n       [0.78932214],\n       [0.7919693 ],\n       [0.791167  ],\n       [0.79034007],\n       [0.78320825],\n       [0.78254616],\n       [0.7874644 ],\n       [0.7934761 ],\n       [0.78571224],\n       [0.7872106 ],\n       [0.7910187 ],\n       [0.7902682 ],\n       [0.79329324],\n       [0.7968813 ],\n       [0.7982702 ],\n       [0.80499744],\n       [0.8033354 ],\n       [0.8039007 ],\n       [0.80226207],\n       [0.80207837],\n       [0.8084837 ],\n       [0.8224803 ],\n       [0.81501925],\n       [0.8221921 ],\n       [0.81922114],\n       [0.81298375],\n       [0.80998564],\n       [0.8093419 ],\n       [0.8088542 ],\n       [0.8118384 ],\n       [0.8160949 ],\n       [0.8189951 ],\n       [0.8239467 ],\n       [0.83098197],\n       [0.83568037],\n       [0.836825  ],\n       [0.83579254],\n       [0.8355942 ],\n       [0.83058393],\n       [0.83941376],\n       [0.8385371 ],\n       [0.8370849 ],\n       [0.83566105],\n       [0.8409225 ],\n       [0.836465  ],\n       [0.8402904 ],\n       [0.8395959 ],\n       [0.83738565],\n       [0.838886  ],\n       [0.84180415],\n       [0.8389294 ],\n       [0.8357254 ],\n       [0.8359002 ],\n       [0.83832765],\n       [0.8395175 ],\n       [0.83958673],\n       [0.83714855],\n       [0.8428633 ],\n       [0.8448764 ],\n       [0.84568   ],\n       [0.84120154],\n       [0.84519494],\n       [0.8432025 ],\n       [0.8426851 ],\n       [0.84299064],\n       [0.84595966],\n       [0.84089243],\n       [0.842183  ],\n       [0.8408035 ],\n       [0.8430146 ],\n       [0.8450413 ],\n       [0.84954906],\n       [0.853243  ],\n       [0.8588675 ],\n       [0.85840833],\n       [0.8565979 ],\n       [0.8573557 ],\n       [0.8604164 ],\n       [0.8518915 ],\n       [0.8518443 ],\n       [0.8620368 ],\n       [0.86885643],\n       [0.86125565],\n       [0.8672925 ],\n       [0.86987746],\n       [0.86779857],\n       [0.8602145 ],\n       [0.8593756 ],\n       [0.8457961 ],\n       [0.8493937 ],\n       [0.84923303],\n       [0.85389614],\n       [0.85239565],\n       [0.85091174],\n       [0.8513931 ],\n       [0.85111344],\n       [0.8549272 ],\n       [0.8580067 ],\n       [0.85643137],\n       [0.8547596 ],\n       [0.8575772 ],\n       [0.85523295],\n       [0.8578497 ],\n       [0.8519783 ],\n       [0.85413074],\n       [0.8520589 ],\n       [0.86014855],\n       [0.86020684],\n       [0.8679546 ],\n       [0.87255716],\n       [0.8705958 ],\n       [0.87615037],\n       [0.87683666],\n       [0.8814783 ],\n       [0.87644553],\n       [0.8753356 ],\n       [0.8750824 ],\n       [0.8700446 ],\n       [0.8662449 ],\n       [0.8621502 ],\n       [0.8589375 ],\n       [0.8586972 ],\n       [0.86358714],\n       [0.8769125 ],\n       [0.88284826],\n       [0.88271105],\n       [0.8805386 ],\n       [0.88322484],\n       [0.884037  ],\n       [0.8855479 ],\n       [0.88658977],\n       [0.89424753],\n       [0.8967407 ],\n       [0.89972675],\n       [0.8971263 ],\n       [0.89208925],\n       [0.8900777 ],\n       [0.8887681 ],\n       [0.89623976],\n       [0.9033215 ],\n       [0.8972018 ],\n       [0.8985611 ],\n       [0.8955457 ],\n       [0.8948225 ],\n       [0.8955946 ],\n       [0.9002937 ],\n       [0.8968288 ],\n       [0.8942311 ],\n       [0.89480484],\n       [0.89315176],\n       [0.8976393 ],\n       [0.8946228 ],\n       [0.8911568 ],\n       [0.89093304],\n       [0.890584  ],\n       [0.88743424],\n       [0.8911207 ],\n       [0.8846748 ],\n       [0.8905184 ],\n       [0.89386284],\n       [0.8845053 ],\n       [0.88575244],\n       [0.892172  ],\n       [0.89140356],\n       [0.87981284],\n       [0.87834096],\n       [0.8799342 ],\n       [0.8805635 ],\n       [0.87263   ],\n       [0.87310076],\n       [0.86853373],\n       [0.87350667],\n       [0.87547624],\n       [0.87555933],\n       [0.87596035],\n       [0.8655437 ],\n       [0.8694024 ],\n       [0.8701117 ],\n       [0.87165153],\n       [0.8814851 ],\n       [0.8864527 ],\n       [0.88812566],\n       [0.88411224],\n       [0.8940195 ],\n       [0.89802337],\n       [0.89793444],\n       [0.8978796 ],\n       [0.89471793],\n       [0.8980802 ],\n       [0.900972  ],\n       [0.9056778 ],\n       [0.90339136],\n       [0.9030926 ],\n       [0.90322423],\n       [0.8991531 ],\n       [0.9070941 ],\n       [0.9043504 ],\n       [0.9114983 ],\n       [0.9149958 ],\n       [0.91917396],\n       [0.9118446 ],\n       [0.9125459 ],\n       [0.91978407],\n       [0.9048816 ],\n       [0.8932748 ],\n       [0.88097394],\n       [0.8649355 ],\n       [0.8672402 ],\n       [0.85553575],\n       [0.86023724],\n       [0.8633417 ],\n       [0.8679869 ],\n       [0.86857045],\n       [0.8553406 ],\n       [0.8557464 ],\n       [0.8571652 ],\n       [0.87041295],\n       [0.8760252 ],\n       [0.8724532 ],\n       [0.8647677 ],\n       [0.8602576 ],\n       [0.8549999 ],\n       [0.8657149 ],\n       [0.8628235 ],\n       [0.8673049 ],\n       [0.8704417 ],\n       [0.876441  ],\n       [0.87989855],\n       [0.8784833 ],\n       [0.87965965],\n       [0.87991524],\n       [0.8764175 ],\n       [0.8793756 ],\n       [0.8810768 ],\n       [0.88286746],\n       [0.86068285],\n       [0.86568093],\n       [0.8674288 ],\n       [0.8633137 ],\n       [0.87023604],\n       [0.87282753],\n       [0.8748591 ],\n       [0.87166893],\n       [0.8615558 ],\n       [0.8720025 ],\n       [0.87051713],\n       [0.87321115],\n       [0.87435865],\n       [0.87497115],\n       [0.873435  ],\n       [0.8758682 ],\n       [0.8751644 ],\n       [0.8764732 ],\n       [0.88703644],\n       [0.88596797],\n       [0.88048744],\n       [0.8809247 ],\n       [0.876822  ],\n       [0.87134576],\n       [0.87903225],\n       [0.8822272 ],\n       [0.88884866],\n       [0.8898823 ],\n       [0.8851042 ],\n       [0.87643695],\n       [0.8739908 ],\n       [0.8717346 ],\n       [0.87121844],\n       [0.8752605 ],\n       [0.8749716 ],\n       [0.86599326],\n       [0.8674704 ],\n       [0.8682163 ],\n       [0.87461746],\n       [0.8766738 ],\n       [0.8731166 ],\n       [0.8710722 ],\n       [0.871315  ],\n       [0.87663853],\n       [0.87250304],\n       [0.8622265 ],\n       [0.86797893],\n       [0.87167895],\n       [0.8705869 ],\n       [0.8731593 ],\n       [0.877197  ],\n       [0.87486005],\n       [0.8774706 ],\n       [0.8759248 ],\n       [0.86637676],\n       [0.86755824],\n       [0.86409116],\n       [0.85498035],\n       [0.8594971 ],\n       [0.8527795 ],\n       [0.8527826 ],\n       [0.85027325],\n       [0.84722567],\n       [0.8423834 ],\n       [0.8336003 ],\n       [0.8406944 ],\n       [0.8278934 ],\n       [0.8260157 ],\n       [0.82532895],\n       [0.82248306],\n       [0.8253895 ],\n       [0.82882035],\n       [0.8315867 ],\n       [0.8262507 ],\n       [0.82570815],\n       [0.8313035 ],\n       [0.8256924 ],\n       [0.8275006 ],\n       [0.8210932 ],\n       [0.8132504 ],\n       [0.8168415 ],\n       [0.80492926],\n       [0.8055192 ],\n       [0.8043852 ],\n       [0.810647  ],\n       [0.81019104],\n       [0.8093325 ],\n       [0.81208956],\n       [0.82001615],\n       [0.809692  ],\n       [0.8131181 ],\n       [0.81147635],\n       [0.8138858 ],\n       [0.8126918 ],\n       [0.8109778 ],\n       [0.80640817],\n       [0.7980993 ],\n       [0.79808795],\n       [0.7934457 ],\n       [0.7966758 ],\n       [0.799337  ],\n       [0.8027438 ],\n       [0.79901624],\n       [0.79653573],\n       [0.7987292 ],\n       [0.80685854],\n       [0.8100456 ],\n       [0.81524265],\n       [0.813236  ],\n       [0.819577  ],\n       [0.8175298 ],\n       [0.81967914],\n       [0.81161654],\n       [0.80939543],\n       [0.8084303 ],\n       [0.8113302 ],\n       [0.8082893 ],\n       [0.80634975],\n       [0.8085029 ],\n       [0.81844604],\n       [0.8146384 ],\n       [0.81494534],\n       [0.8184092 ],\n       [0.82354116],\n       [0.82480264],\n       [0.82829976],\n       [0.82675433],\n       [0.82750094],\n       [0.8194226 ],\n       [0.8116442 ],\n       [0.8133342 ],\n       [0.80996335],\n       [0.8061372 ],\n       [0.7817972 ],\n       [0.7872937 ],\n       [0.7787107 ],\n       [0.7751888 ],\n       [0.7814448 ],\n       [0.7811943 ],\n       [0.7853668 ],\n       [0.78539777],\n       [0.76968956],\n       [0.76796067],\n       [0.75936747],\n       [0.75067604],\n       [0.7431724 ],\n       [0.7450458 ],\n       [0.748214  ],\n       [0.7489164 ],\n       [0.7710742 ],\n       [0.76661766],\n       [0.7688439 ],\n       [0.7601336 ],\n       [0.7624055 ],\n       [0.75899935],\n       [0.7592137 ],\n       [0.75377953],\n       [0.752849  ],\n       [0.75709236],\n       [0.7556411 ],\n       [0.75595903],\n       [0.75079584],\n       [0.7479446 ],\n       [0.7482718 ],\n       [0.74876523],\n       [0.75741947],\n       [0.7583399 ],\n       [0.75562716],\n       [0.7598481 ],\n       [0.7523179 ],\n       [0.75973475],\n       [0.75522506],\n       [0.7518045 ],\n       [0.7442732 ],\n       [0.7464701 ],\n       [0.74367213],\n       [0.74731827],\n       [0.75232255],\n       [0.7548318 ],\n       [0.75053656],\n       [0.74992   ],\n       [0.7476723 ],\n       [0.7543341 ],\n       [0.7501005 ],\n       [0.74942124],\n       [0.74400043],\n       [0.7336439 ],\n       [0.7338475 ],\n       [0.73954296]], dtype=float32), array([[0.11302913, 0.22747437, 0.54840163, ..., 0.10606061, 0.29517134,\n        0.27119701],\n       [0.11082609, 0.22451696, 0.54807282, ..., 0.10606061, 0.30218069,\n        0.27899002],\n       [0.11706301, 0.22353115, 0.55324849, ..., 0.10606061, 0.30529595,\n        0.28288653],\n       ...,\n       [0.13201151, 0.34542587, 0.60074286, ..., 0.09090909, 0.29906542,\n        0.2236596 ],\n       [0.12889305, 0.33433557, 0.60305669, ..., 0.07575758, 0.3029595 ,\n        0.23145262],\n       [0.1312168 , 0.33763801, 0.60884126, ..., 0.07575758, 0.28816199,\n        0.2213217 ]]))]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                 \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    696\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[1;32m--> 697\u001b[1;33m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m                          'Expected a symbolic tensor instance.')\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'tuple'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-da23b6fe923a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# invert scaling for forecast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0minv_yhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0minv_yhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minv_yhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0minv_yhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minv_yhat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# invert scaling for actual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\merge.py\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconcatenation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0malongside\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \"\"\"\n\u001b[1;32m--> 649\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    314\u001b[0m                                  \u001b[1;34m'Received type: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full input: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                                  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. All inputs to the layer '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m                                  'should be tensors.')\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer concatenate_10 was called with an input that isn't a symbolic tensor. Received type: <class 'tuple'>. Full input: [(array([[0.7570466 ],\n       [0.7682475 ],\n       [0.7762338 ],\n       [0.77217567],\n       [0.7674738 ],\n       [0.76899123],\n       [0.7722814 ],\n       [0.77488005],\n       [0.7766211 ],\n       [0.7775085 ],\n       [0.77784705],\n       [0.7725867 ],\n       [0.7738311 ],\n       [0.7684125 ],\n       [0.77673686],\n       [0.7752683 ],\n       [0.77895474],\n       [0.7782291 ],\n       [0.7765517 ],\n       [0.776726  ],\n       [0.7762456 ],\n       [0.7770226 ],\n       [0.7781091 ],\n       [0.7771394 ],\n       [0.7824124 ],\n       [0.78234065],\n       [0.78467107],\n       [0.7862673 ],\n       [0.789443  ],\n       [0.79018676],\n       [0.7917814 ],\n       [0.78898954],\n       [0.7860291 ],\n       [0.7881701 ],\n       [0.79067934],\n       [0.7832844 ],\n       [0.78240407],\n       [0.78532517],\n       [0.78723836],\n       [0.78132427],\n       [0.7797041 ],\n       [0.7837982 ],\n       [0.78420436],\n       [0.77672017],\n       [0.7793063 ],\n       [0.7810577 ],\n       [0.77905595],\n       [0.78529227],\n       [0.78571105],\n       [0.79057133],\n       [0.793965  ],\n       [0.7901119 ],\n       [0.7909374 ],\n       [0.7891859 ],\n       [0.78872967],\n       [0.78789616],\n       [0.7899612 ],\n       [0.78932214],\n       [0.7919693 ],\n       [0.791167  ],\n       [0.79034007],\n       [0.78320825],\n       [0.78254616],\n       [0.7874644 ],\n       [0.7934761 ],\n       [0.78571224],\n       [0.7872106 ],\n       [0.7910187 ],\n       [0.7902682 ],\n       [0.79329324],\n       [0.7968813 ],\n       [0.7982702 ],\n       [0.80499744],\n       [0.8033354 ],\n       [0.8039007 ],\n       [0.80226207],\n       [0.80207837],\n       [0.8084837 ],\n       [0.8224803 ],\n       [0.81501925],\n       [0.8221921 ],\n       [0.81922114],\n       [0.81298375],\n       [0.80998564],\n       [0.8093419 ],\n       [0.8088542 ],\n       [0.8118384 ],\n       [0.8160949 ],\n       [0.8189951 ],\n       [0.8239467 ],\n       [0.83098197],\n       [0.83568037],\n       [0.836825  ],\n       [0.83579254],\n       [0.8355942 ],\n       [0.83058393],\n       [0.83941376],\n       [0.8385371 ],\n       [0.8370849 ],\n       [0.83566105],\n       [0.8409225 ],\n       [0.836465  ],\n       [0.8402904 ],\n       [0.8395959 ],\n       [0.83738565],\n       [0.838886  ],\n       [0.84180415],\n       [0.8389294 ],\n       [0.8357254 ],\n       [0.8359002 ],\n       [0.83832765],\n       [0.8395175 ],\n       [0.83958673],\n       [0.83714855],\n       [0.8428633 ],\n       [0.8448764 ],\n       [0.84568   ],\n       [0.84120154],\n       [0.84519494],\n       [0.8432025 ],\n       [0.8426851 ],\n       [0.84299064],\n       [0.84595966],\n       [0.84089243],\n       [0.842183  ],\n       [0.8408035 ],\n       [0.8430146 ],\n       [0.8450413 ],\n       [0.84954906],\n       [0.853243  ],\n       [0.8588675 ],\n       [0.85840833],\n       [0.8565979 ],\n       [0.8573557 ],\n       [0.8604164 ],\n       [0.8518915 ],\n       [0.8518443 ],\n       [0.8620368 ],\n       [0.86885643],\n       [0.86125565],\n       [0.8672925 ],\n       [0.86987746],\n       [0.86779857],\n       [0.8602145 ],\n       [0.8593756 ],\n       [0.8457961 ],\n       [0.8493937 ],\n       [0.84923303],\n       [0.85389614],\n       [0.85239565],\n       [0.85091174],\n       [0.8513931 ],\n       [0.85111344],\n       [0.8549272 ],\n       [0.8580067 ],\n       [0.85643137],\n       [0.8547596 ],\n       [0.8575772 ],\n       [0.85523295],\n       [0.8578497 ],\n       [0.8519783 ],\n       [0.85413074],\n       [0.8520589 ],\n       [0.86014855],\n       [0.86020684],\n       [0.8679546 ],\n       [0.87255716],\n       [0.8705958 ],\n       [0.87615037],\n       [0.87683666],\n       [0.8814783 ],\n       [0.87644553],\n       [0.8753356 ],\n       [0.8750824 ],\n       [0.8700446 ],\n       [0.8662449 ],\n       [0.8621502 ],\n       [0.8589375 ],\n       [0.8586972 ],\n       [0.86358714],\n       [0.8769125 ],\n       [0.88284826],\n       [0.88271105],\n       [0.8805386 ],\n       [0.88322484],\n       [0.884037  ],\n       [0.8855479 ],\n       [0.88658977],\n       [0.89424753],\n       [0.8967407 ],\n       [0.89972675],\n       [0.8971263 ],\n       [0.89208925],\n       [0.8900777 ],\n       [0.8887681 ],\n       [0.89623976],\n       [0.9033215 ],\n       [0.8972018 ],\n       [0.8985611 ],\n       [0.8955457 ],\n       [0.8948225 ],\n       [0.8955946 ],\n       [0.9002937 ],\n       [0.8968288 ],\n       [0.8942311 ],\n       [0.89480484],\n       [0.89315176],\n       [0.8976393 ],\n       [0.8946228 ],\n       [0.8911568 ],\n       [0.89093304],\n       [0.890584  ],\n       [0.88743424],\n       [0.8911207 ],\n       [0.8846748 ],\n       [0.8905184 ],\n       [0.89386284],\n       [0.8845053 ],\n       [0.88575244],\n       [0.892172  ],\n       [0.89140356],\n       [0.87981284],\n       [0.87834096],\n       [0.8799342 ],\n       [0.8805635 ],\n       [0.87263   ],\n       [0.87310076],\n       [0.86853373],\n       [0.87350667],\n       [0.87547624],\n       [0.87555933],\n       [0.87596035],\n       [0.8655437 ],\n       [0.8694024 ],\n       [0.8701117 ],\n       [0.87165153],\n       [0.8814851 ],\n       [0.8864527 ],\n       [0.88812566],\n       [0.88411224],\n       [0.8940195 ],\n       [0.89802337],\n       [0.89793444],\n       [0.8978796 ],\n       [0.89471793],\n       [0.8980802 ],\n       [0.900972  ],\n       [0.9056778 ],\n       [0.90339136],\n       [0.9030926 ],\n       [0.90322423],\n       [0.8991531 ],\n       [0.9070941 ],\n       [0.9043504 ],\n       [0.9114983 ],\n       [0.9149958 ],\n       [0.91917396],\n       [0.9118446 ],\n       [0.9125459 ],\n       [0.91978407],\n       [0.9048816 ],\n       [0.8932748 ],\n       [0.88097394],\n       [0.8649355 ],\n       [0.8672402 ],\n       [0.85553575],\n       [0.86023724],\n       [0.8633417 ],\n       [0.8679869 ],\n       [0.86857045],\n       [0.8553406 ],\n       [0.8557464 ],\n       [0.8571652 ],\n       [0.87041295],\n       [0.8760252 ],\n       [0.8724532 ],\n       [0.8647677 ],\n       [0.8602576 ],\n       [0.8549999 ],\n       [0.8657149 ],\n       [0.8628235 ],\n       [0.8673049 ],\n       [0.8704417 ],\n       [0.876441  ],\n       [0.87989855],\n       [0.8784833 ],\n       [0.87965965],\n       [0.87991524],\n       [0.8764175 ],\n       [0.8793756 ],\n       [0.8810768 ],\n       [0.88286746],\n       [0.86068285],\n       [0.86568093],\n       [0.8674288 ],\n       [0.8633137 ],\n       [0.87023604],\n       [0.87282753],\n       [0.8748591 ],\n       [0.87166893],\n       [0.8615558 ],\n       [0.8720025 ],\n       [0.87051713],\n       [0.87321115],\n       [0.87435865],\n       [0.87497115],\n       [0.873435  ],\n       [0.8758682 ],\n       [0.8751644 ],\n       [0.8764732 ],\n       [0.88703644],\n       [0.88596797],\n       [0.88048744],\n       [0.8809247 ],\n       [0.876822  ],\n       [0.87134576],\n       [0.87903225],\n       [0.8822272 ],\n       [0.88884866],\n       [0.8898823 ],\n       [0.8851042 ],\n       [0.87643695],\n       [0.8739908 ],\n       [0.8717346 ],\n       [0.87121844],\n       [0.8752605 ],\n       [0.8749716 ],\n       [0.86599326],\n       [0.8674704 ],\n       [0.8682163 ],\n       [0.87461746],\n       [0.8766738 ],\n       [0.8731166 ],\n       [0.8710722 ],\n       [0.871315  ],\n       [0.87663853],\n       [0.87250304],\n       [0.8622265 ],\n       [0.86797893],\n       [0.87167895],\n       [0.8705869 ],\n       [0.8731593 ],\n       [0.877197  ],\n       [0.87486005],\n       [0.8774706 ],\n       [0.8759248 ],\n       [0.86637676],\n       [0.86755824],\n       [0.86409116],\n       [0.85498035],\n       [0.8594971 ],\n       [0.8527795 ],\n       [0.8527826 ],\n       [0.85027325],\n       [0.84722567],\n       [0.8423834 ],\n       [0.8336003 ],\n       [0.8406944 ],\n       [0.8278934 ],\n       [0.8260157 ],\n       [0.82532895],\n       [0.82248306],\n       [0.8253895 ],\n       [0.82882035],\n       [0.8315867 ],\n       [0.8262507 ],\n       [0.82570815],\n       [0.8313035 ],\n       [0.8256924 ],\n       [0.8275006 ],\n       [0.8210932 ],\n       [0.8132504 ],\n       [0.8168415 ],\n       [0.80492926],\n       [0.8055192 ],\n       [0.8043852 ],\n       [0.810647  ],\n       [0.81019104],\n       [0.8093325 ],\n       [0.81208956],\n       [0.82001615],\n       [0.809692  ],\n       [0.8131181 ],\n       [0.81147635],\n       [0.8138858 ],\n       [0.8126918 ],\n       [0.8109778 ],\n       [0.80640817],\n       [0.7980993 ],\n       [0.79808795],\n       [0.7934457 ],\n       [0.7966758 ],\n       [0.799337  ],\n       [0.8027438 ],\n       [0.79901624],\n       [0.79653573],\n       [0.7987292 ],\n       [0.80685854],\n       [0.8100456 ],\n       [0.81524265],\n       [0.813236  ],\n       [0.819577  ],\n       [0.8175298 ],\n       [0.81967914],\n       [0.81161654],\n       [0.80939543],\n       [0.8084303 ],\n       [0.8113302 ],\n       [0.8082893 ],\n       [0.80634975],\n       [0.8085029 ],\n       [0.81844604],\n       [0.8146384 ],\n       [0.81494534],\n       [0.8184092 ],\n       [0.82354116],\n       [0.82480264],\n       [0.82829976],\n       [0.82675433],\n       [0.82750094],\n       [0.8194226 ],\n       [0.8116442 ],\n       [0.8133342 ],\n       [0.80996335],\n       [0.8061372 ],\n       [0.7817972 ],\n       [0.7872937 ],\n       [0.7787107 ],\n       [0.7751888 ],\n       [0.7814448 ],\n       [0.7811943 ],\n       [0.7853668 ],\n       [0.78539777],\n       [0.76968956],\n       [0.76796067],\n       [0.75936747],\n       [0.75067604],\n       [0.7431724 ],\n       [0.7450458 ],\n       [0.748214  ],\n       [0.7489164 ],\n       [0.7710742 ],\n       [0.76661766],\n       [0.7688439 ],\n       [0.7601336 ],\n       [0.7624055 ],\n       [0.75899935],\n       [0.7592137 ],\n       [0.75377953],\n       [0.752849  ],\n       [0.75709236],\n       [0.7556411 ],\n       [0.75595903],\n       [0.75079584],\n       [0.7479446 ],\n       [0.7482718 ],\n       [0.74876523],\n       [0.75741947],\n       [0.7583399 ],\n       [0.75562716],\n       [0.7598481 ],\n       [0.7523179 ],\n       [0.75973475],\n       [0.75522506],\n       [0.7518045 ],\n       [0.7442732 ],\n       [0.7464701 ],\n       [0.74367213],\n       [0.74731827],\n       [0.75232255],\n       [0.7548318 ],\n       [0.75053656],\n       [0.74992   ],\n       [0.7476723 ],\n       [0.7543341 ],\n       [0.7501005 ],\n       [0.74942124],\n       [0.74400043],\n       [0.7336439 ],\n       [0.7338475 ],\n       [0.73954296]], dtype=float32), array([[0.11302913, 0.22747437, 0.54840163, ..., 0.10606061, 0.29517134,\n        0.27119701],\n       [0.11082609, 0.22451696, 0.54807282, ..., 0.10606061, 0.30218069,\n        0.27899002],\n       [0.11706301, 0.22353115, 0.55324849, ..., 0.10606061, 0.30529595,\n        0.28288653],\n       ...,\n       [0.13201151, 0.34542587, 0.60074286, ..., 0.09090909, 0.29906542,\n        0.2236596 ],\n       [0.12889305, 0.33433557, 0.60305669, ..., 0.07575758, 0.3029595 ,\n        0.23145262],\n       [0.1312168 , 0.33763801, 0.60884126, ..., 0.07575758, 0.28816199,\n        0.2213217 ]]))]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
